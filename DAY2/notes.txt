Kubernetes cluster
==================

1. self managed k8s cluster
===========================

   1. minikube [single node k8s cluster]

   2. kubeadm [multi node k8s cluster]

--> If any failure in the pod, It will taken care by k8s. If any failure in the nodes, k8s will not handle manually. we need to fix it


2. cloud managed k8s cluster
============================

1. AWS ---> EKS [Elastic k8s service]

2. Azure --> AKS [Azure k8s service]

3. Google --> GKE [Google k8s engine]


--> If any failure in the pod, It will taken care by k8s. If any failure in the nodes, cloud will  handle manually. we need to fix it


1 master

2 salve nodes






=============================================================================================================================
KK FUNDA  Ph: 863980177/9676831734
==============================================================================================================================

Agenda: Kubernetes Setup Using Kubeadm In AWS EC2 Ubuntu Servers Container-D As Runtime
=======

Prerequisite:
=============

3 - Ubuntu Serves

1 - master  (4GB RAM , 2 Core) t2.medium

2 - Workers  (1 GB, 1 Core)     t2.micro


Note: Open Required Ports In AWS Security Groups. For now we will open All trafic.

==========COMMON FOR MASTER & SLAVES START ====

1) Switch to root user
   
sudo su -


2) Disable swap & add kernel settings

#https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
=======================================================================================

swapoff -a
sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab


3) Add  kernel settings & Enable IP tables(CNI Prerequisites)-Communication b/w POD to POD

https://kubernetes.io/docs/setup/production-environment/container-runtimes/
============================================================================

cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF


modprobe overlay
modprobe br_netfilter



cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF


sysctl --system

4) Install containerd run time

https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd
=======================================================================================

https://github.com/containerd/containerd/blob/main/docs/getting-started.md
==========================================================================

https://docs.docker.com/engine/install/ubuntu/
==============================================

#To install containerd, first install its dependencies.

apt-get update -y 
apt-get install ca-certificates curl gnupg lsb-release -y



Note: We are not installing Docker Here.Since containerd.io package is part of docker apt repositories hence we added docker repository & it's key to download and install containerd.


# Add Docker’s official GPG key:
mkdir -p /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg

#Use follwing command to set up the repository:

echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
  
  
# Install containerd

apt-get update -y
apt-get install containerd.io -y

# Generate default configuration file for containerd

Note: Containerd uses a configuration file located in /etc/containerd/config.toml for specifying daemon level options.
The default configuration can be generated via below command.

containerd config default > /etc/containerd/config.toml

# Run following command to update configure cgroup as systemd for contianerd.

sed -i 's/SystemdCgroup \= false/SystemdCgroup \= true/g' /etc/containerd/config.toml

# Restart and enable containerd service

systemctl restart containerd
systemctl enable containerd


5) Installing kubeadm, kubelet and kubectl 

# Update the apt package index and install packages needed to use the Kubernetes apt repository:

https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#check-required-ports
===========================================================================================================

apt-get update
apt-get install -y apt-transport-https ca-certificates curl

# Download the Google Cloud public signing key:



curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

# Add the Kubernetes apt repository:



echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list



# Update apt package index, install kubelet, kubeadm and kubectl, and pin their version:

apt-get update
apt-get install -y kubelet kubeadm kubectl

# apt-mark hold will prevent the package from being automatically upgraded or removed.
apt-mark hold kubelet kubeadm kubectl


# Enable and start kubelet service

systemctl daemon-reload 
systemctl start kubelet 
systemctl enable kubelet.service


==========COMMON FOR MASTER & SLAVES END ====



   
===========In Master Node Start====================

https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/
==============================================================================================


# Steps Only For Kubernetes Master

# Switch to the root user.

sudo su -

# Initialize Kubernates master by executing below commond.

kubeadm init

# If you want to initialize kubernetes on Public EndPoint(Not recommended in real time). You can use below option Replace PUBLIC_IP with actual public ip of your kubernetes master node (Recommended to use Elastic(Create and assign elastic IP to master node and use that Elastic IP below)).Replace PORT with 6443 (API Server Port). 

#kubeadm init --control-plane-endpoint "PUBLIC_IP:PORT"

IF Error
#sudo kubeadm init --cri-socket /run/containerd/containerd.sock


# Configure kubectl  exit as root user & exeucte as normal ubuntu user
exit

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config


# To verify, if kubectl is working or not, run the following command.

kubectl get nodes

kubectl get pods -o wide -n kube-system

#You will notice from the previous command, that all the pods are running except one: ‘core-dns’. For resolving this we will install a # pod network addon like Calico or Weavenet ..etc. 


Note: Install any one network addon don't install both. Install either weave net or calico.

https://kubernetes.io/docs/concepts/cluster-administration/addons/
==================================================================


To install Weave net run the following command.

https://www.weave.works/docs/net/latest/kubernetes/kube-addon/
================================================================

kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml

Or To install the calico network addon, run the following command:

#kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml 


kubectl get nodes

kubectl get pods 
kubectl get pods --all-namespaces


# Get token

kubeadm token create --print-join-command

=========In Master Node End====================


Add Worker Machines to Kubernates Master
=========================================

Copy kubeadm join token from and execute in Worker Nodes to join to cluster



kubectl commonds has to be executed in master machine.

Check Nodes 
=============

kubectl get nodes



Deploy Sample Application
==========================

kubectl run nginx-demo --image=nginx --port=80 

kubectl expose pod nginx-demo --port=80 --target-port=80 --type=NodePort --namespace=prod


Get Node Port details 
=====================
kubectl get services


===================================================================================================


Kubernetes Objects  / API resources
====================================

Pod
ReplicationController
ReplicaSet
DaemonSet
Deployment
StatefulSet

namespace



ConfigMap
Secret

Service
NetworkPolicies
Ingress


PersistentVolumeClaim
PersistentVolume




what is cluster?
================

It is group of k8s servers/nodes.

what is node?
=============


How to list all k8s resources?
==============================

kubectl api-resources




kubectl create namespace <NN>

kubectl create ns testing-ns



k8s nameSpaces
==============
In Kubernetes, namespaces are a way to organize and manage resources within a cluster. They help you divide cluster resources between multiple users or teams and can be useful for managing resource quotas, network policies, and access control.

Here are some key commands and YAML examples related to Kubernetes namespaces:

1. Creating a Namespace
------------------------


Create a namespace using a YAML file, as shown earlier:

apiVersion: v1
kind: Namespace
metadata:
  name: my-namespace
  labels:
    name: my-namespace

kubectl apply -f namespace.yaml


--> Alternatively, you can create a namespace directly from the command line:

kubectl create namespace my-namespace








2. Listing Namespaces
---------------------

kubectl get namespaces  or kubectl get ns


3. Viewing Namespace Details
-----------------------------

To get detailed information about a specific namespace:

kubectl describe ns my-namespace







4. Deleting a Namespace
-----------------------

To delete a namespace (along with all the resources within it):

kubectl delete namespace my-namespace








5. Using Namespaces in Resource Definitions
---------------------------------------------


When defining resources (like Pods, Services, Deployments) in YAML files, you specify the namespace under the metadata section:


apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  namespace: my-namespace
spec:
  containers:
  - name: ngnix123
    image: nginx



kubectl apply -f pod.yaml


6. Switching Namespaces in kubectl Context
--------------------------------------------

kubectl config set-context --current --namespace=prod

7. Namespace Resource Quotas
-----------------------------

You can define resource quotas and limits per namespace to control the usage of resources:

apiVersion: v1
kind: ResourceQuota
metadata:
  name: my-resource-quota
  namespace: prod
spec:
  hard:
    pods: "10"


kubectl apply -f resource-quota.yaml
